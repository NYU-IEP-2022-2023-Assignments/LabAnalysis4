{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOdasvU9JDDl9s7IDVq6fx9",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NYU-IEP-2022-2023-Assignments/LabAnalysis4/blob/main/LabAnalysis4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QpswBbfWcR0E"
      },
      "source": [
        "#Initial library includes and installations\n",
        "run once - does not require you to edit anything"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uHTWtceOIVdV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1c48a7cf-e164-4d1f-e166-539108b4bbeb"
      },
      "source": [
        "!pip install munch\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os, glob\n",
        "from munch import munchify\n",
        "import scipy.stats\n",
        "from sklearn import linear_model, datasets\n",
        "\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting munch\n",
            "  Downloading munch-2.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from munch) (1.15.0)\n",
            "Installing collected packages: munch\n",
            "Successfully installed munch-2.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGierwNjcdQa"
      },
      "source": [
        "#Function definitions and constants\n",
        "Run once - does not require you to edit anything\n",
        "\n",
        "These functions are provided for you - see function definitions and comments for more information on their return values and usage\n",
        "\n",
        "  1. `loadDataSet(filename)` - loads an individual .json file and checks it for really large jumps in major axis angle, which would indicate a problem with the fits; \"unwraps\" the angle to eliminate discontinuities, for example when the angle crosses from -2$\\pi$ to 0.  \n",
        "  1. `loadAllDataSets(startdir)` - loads all json files in a directory\n",
        "  1. `meanOverTime(t,y,deltat)` - breaks data up into chunks about delta t long and calculates the average value of y over each chunk (assumes t is evenly spaced and monotonically increasing)\n",
        "  1. `rateOfChange (t,y)` - calculates dy/dt (assumes t is monotonically increasing)\n",
        "  1. `m,b,m_e,b_e = fitLine(x,y)` - least squares fit to $y = (m\\pm m_e)x + b\\pm b_e$ $m_e$ and $b_e$ are the uncertainties (errors) in the estimates of $m$ and $b$\n",
        "  1. `m,b,me,be,xi,yi,xo,yo = fitLineRansac(x,y)` - least squares fit to y = m x + b , discarding outliers - in addition to m, b, m_e and b_e, this function returns inlying (xi,yi) and outlying (xo,yo) points : uses [RANSAC](https://https://en.wikipedia.org/wiki/Random_sample_consensus) to find outliers\n",
        "  1. `m,b,me,be,xi,yi,xo,yo = fitLineHuber(x,y)` - least squares fit to y = m x + b , discarding outliers - in addition to m, b, m_e and b_e, this function returns inlying (xi,yi) and outlying (xo,yo) points : uses [Huber](https://scikit-learn.org/stable/modules/linear_model.html#huber-regression) to find outliers\n",
        "\n",
        "Here we also define some useful constants:\n",
        "\n",
        "`deghr` multiply by deghr to convert radians per second to degrees per hour\n",
        "\n",
        "`omega_foucault` predicted orbit precession (in radians per second) due to earth's rotation, at NYC latitude\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3wquqyKbOpM"
      },
      "source": [
        "gaccel = 9.802\n",
        "\n",
        "nyc_latitude =40.730610\n",
        "deghr = np.rad2deg(3600)\n",
        "omega_foucault = -15*np.sin(np.deg2rad(nyc_latitude))/deghr\n",
        "\n",
        "\n",
        "#loads one data set (work from one lab group stored in a .json file)\n",
        "#data can be accessed as either a dictionary or a structure\n",
        "#ie dataset[\"fits\"] and dataset.fits are the same thing\n",
        "#setupNumber: number on the wall\n",
        "#section: 1-4 which section data came from\n",
        "#thetaWall: angle of a line parallel to the wall\n",
        "#notes\n",
        "#filename: name of the json file\n",
        "#fits: list of fits to individual trials\n",
        "#   filename: path to data file\n",
        "#   period: period of the pendulum, inferred from the data\n",
        "#   length: length of the pendulum (in mm), inferred from the period\n",
        "#   ##(N,) arrays with orbit parameters##\n",
        "#     number: crossing number at the start of each orbit (from 0, in steps of 2)\n",
        "#     time: time at the start of each orbit (from 0, in steps of 1 period)\n",
        "#     maxjaxis: fit major axis (in mm) of each orbit\n",
        "#     minaxis: fit minor axis (in mm) of each orbit\n",
        "#     theta: angle (in radians) of the major axis to the x-axis\n",
        "#     slope: slope of the voltage vs. time trace (in V/s) at the crossing - not used in this analysis\n",
        "#   \n",
        "\n",
        "\n",
        "def loadDataSet(filename):\n",
        "  file = open(filename,'r')\n",
        "  results = json.load(file)\n",
        "  file.close\n",
        "  dataset = munchify(results) #can be accessed as a structure or a dict\n",
        "  dataset.filename = filename\n",
        "  dataset.hasAngleToWall = np.abs(dataset.thetaWall) <= 360 #default 9999 is out of range\n",
        "  valid = []\n",
        "  for f in dataset.fits:\n",
        "    try:\n",
        "      f.hasAngletoWall = dataset.hasAngleToWall\n",
        "      f.number = np.array(f.number)\n",
        "      f.time = np.array(f.time)\n",
        "      f.majaxis = np.array(f.majaxis)\n",
        "      f.minaxis = np.array(f.minaxis)\n",
        "      f.theta = np.unwrap(np.array(f.theta))\n",
        "      f.thetaRelToWall = f.theta - np.pi/180*dataset.thetaWall\n",
        "      f.slope = np.array(f.slope)\n",
        "      f.period = 2*(f.time[-1]-f.time[0])/(f.number[-1]-f.number[0])\n",
        "      f.length = 1000*gaccel*(f.period/(2*np.pi))**2\n",
        "      dp = np.diff(np.unwrap(f.theta))\n",
        "      valid.append((np.abs(dp) < np.pi/4).all()) #extremely large jump from one swing to the next - bad fit\n",
        "    except:\n",
        "      valid.append(False)\n",
        "  bad = np.array(valid) == False\n",
        "  if (bad.any()):\n",
        "    print('{}: bad fits found in experiments {}'.format(filename, np.where(bad)[0]))\n",
        "    dataset.fits = [dataset.fits[i] for i in np.where(valid)[0]]\n",
        "  return dataset\n",
        "\n",
        "def loadAllDataSets(startdir):\n",
        "  files = sorted(glob.glob(startdir + '/*.json'))\n",
        "  return [loadDataSet(f) for f in files]\n",
        "\n",
        "#my = meanOverTime (t,y,deltat)\n",
        "#calculates the mean value of y and dy/dt at approximate intervals of deltat\n",
        "#   e.g. if deltat is 10, then my[0] is the average value of y between \n",
        "#   t[0] and t[0] + 10\n",
        "#detalt is adjusted downward to evenly divide the whole range\n",
        "#   e.g. if deltat is 100 and t ranges from 0 to 110 seconds, then delta t will be 55\n",
        "def meanOverTime (t, y, deltat):\n",
        "  numpts = int(np.ceil((t[-1]-t[0])/deltat)) + 1\n",
        "  inds = np.linspace(0,len(t),numpts, endpoint=False, dtype=int)\n",
        "  dt = np.gradient(t)\n",
        "  my = np.diff((np.cumsum(y)*dt)[inds])/np.diff(t[inds])\n",
        "  return my\n",
        "\n",
        "#dy_dt = rateOfChange(t,y)  \n",
        "def rateOfChange (t,y):\n",
        "  return np.gradient(y)/np.gradient(t)\n",
        "\n",
        "\n",
        "#m,b,m_e,b_e = fitLine(x,y)\n",
        "#least squares fit to y = m x + b : m_e,b_e are uncertainties in m,b\n",
        "def fitLine(x,y):\n",
        "  p = np.polyfit(x,y,1)\n",
        "  res = y - p[0]*x - p[1]\n",
        "  m_e = np.sqrt(np.var(res)/(len(x)*np.var(x)))\n",
        "  b_e = np.sqrt(np.mean(x**2))*m_e\n",
        "  return (p[0],p[1],m_e,b_e)\n",
        "  \n",
        "#m,b,me,be,xi,yi,xo,yo = fitLineHuber(x,y)\n",
        "#least squares fit to y = m x + b , discarding outliers\n",
        "#xi,yi are x,y values used (inliers) \n",
        "#xo.yo are x,y values discarded (outliers)\n",
        "def fitLineHuber(x,y,epsilon=2):\n",
        "  huber = linear_model.HuberRegressor(epsilon=epsilon)\n",
        "  huber.fit(x.reshape(-1,1),y)\n",
        "  outlier_mask = huber.outliers_\n",
        "  inlier_mask = np.logical_not(outlier_mask)\n",
        "  (m,b,me,be) = fitLine(x[inlier_mask], y[inlier_mask])\n",
        "  xi = x[inlier_mask]\n",
        "  yi = y[inlier_mask]\n",
        "  xo = x[outlier_mask]\n",
        "  yo = y[outlier_mask]\n",
        "  return (m,b,me,be,xi,yi,xo,yo)\n",
        "\n",
        "#m,b,me,be,xi,yi,xo,yo = fitLineRansac(x,y)\n",
        "#least squares fit to y = m x + b , discarding outliers\n",
        "#xi,yi are x,y values used (inliers) \n",
        "#xo.yo are x,y values discarded (outliers)\n",
        "def fitLineRansac(x,y):\n",
        "  #print(np.median(np.abs(y - np.median(y))))\n",
        "  ransac = linear_model.RANSACRegressor()\n",
        "  ransac.fit(x.reshape(-1,1),y.reshape(-1,1))\n",
        "  inlier_mask = ransac.inlier_mask_\n",
        "  res = y[inlier_mask]-ransac.predict(x.reshape(-1,1))[inlier_mask,0]\n",
        "  ransac.residual_threshold = np.median(np.abs(res))\n",
        "  ransac.fit(x.reshape(-1,1),y.reshape(-1,1))\n",
        "\n",
        "  ransac.stop_probability = 1\n",
        "  ransac.max_trials = 10000\n",
        "  ransac.min_samples = 0.1\n",
        "  res = y[inlier_mask]-ransac.predict(x.reshape(-1,1))[inlier_mask,0]\n",
        "  ransac.residual_threshold = 2.5*np.std(res)\n",
        "  ransac.fit(x.reshape(-1,1),y.reshape(-1,1))\n",
        "  inlier_mask = ransac.inlier_mask_\n",
        "\n",
        "  outlier_mask = np.logical_not(inlier_mask)\n",
        "  (m,b,me,be) = fitLine(x[inlier_mask], y[inlier_mask])\n",
        "  xi = x[inlier_mask]\n",
        "  yi = y[inlier_mask]\n",
        "  xo = x[outlier_mask]\n",
        "  yo = y[outlier_mask]\n",
        "  #print(ransac.n_trials_)\n",
        "  return (m,b,me,be,xi,yi,xo,yo)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CRZ82mnQeKBL"
      },
      "source": [
        "#Fetch the data to be analyzed\n",
        "** Change the first github repository name to match your username (lab4-individual-data-yourname)  **\n",
        "then run once"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fCynEIfRabj6",
        "outputId": "9fdf8caa-8b75-4285-f2d2-5f5625fe70ed"
      },
      "source": [
        "!rm -rf mydata/\n",
        "!rm -rf data/\n",
        "!git clone https://github.com/NYU-IEP-2022-3-Classroom/lab4-data-repository-mgershow  mydata #change to your github username\n",
        "!git clone https://github.com/NYU-IEP-2022-3-Classroom/lab4-shared-data-repository-whole-class data"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'mydata'...\n",
            "remote: Enumerating objects: 56, done.\u001b[K\n",
            "remote: Counting objects: 100% (54/54), done.\u001b[K\n",
            "remote: Compressing objects: 100% (49/49), done.\u001b[K\n",
            "remote: Total 56 (delta 5), reused 50 (delta 4), pack-reused 2\u001b[K\n",
            "Unpacking objects: 100% (56/56), done.\n",
            "Cloning into 'data'...\n",
            "remote: Enumerating objects: 36, done.\u001b[K\n",
            "remote: Counting objects: 100% (36/36), done.\u001b[K\n",
            "remote: Compressing objects: 100% (34/34), done.\u001b[K\n",
            "remote: Total 36 (delta 11), reused 4 (delta 1), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (36/36), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChmLWszotrxC"
      },
      "source": [
        "#load the data sets to be analyzed\n",
        "run once\n",
        "\n",
        "1. `mydata = loadAllDataSets('/content/mydata/')[0]` - load your data -- the [0] is because you only have one data set, so we'll take it out of the list to look at it alone\n",
        "1. `alldata = loadAllDataSets('/content/data/')` - load everyone's data - each element of this list is one dataset\n",
        "\n",
        "When you load the data, you might get a message about bad fits, if someone (not YOU of course) uploaded problematic files to the shared repository. As long as there are only a few of these messages, don't worry about it. \n",
        "\n",
        "## fields in dataset structure\n",
        "- `setupNumber`: number on the wall\n",
        "- `section`: 1-4 which section data came from\n",
        "- `thetaWall`: angle of a line parallel to the wall\n",
        "- `z0`: calibrated magnet height\n",
        "- `filename`: name of the json file\n",
        "- `fits`: list of fits to individual trials\n",
        "   - `anglePath`: path to angle data file\n",
        "   - `magPath`: path to magnetometer data file\n",
        "   - `offset`: (x,y) calculated offset of the magnetometer from 0\n",
        "   - `tiltAngle`: calculated angle the magnet was tilted away from z-axis\n",
        "   - `L`: calculated length of the string (in mm, derived from period)\n",
        "   - `B0`: best estimate of magnetic field strength z0 away directly along magnet axis\n",
        "   - `hasAngleToWall`: whether the original experimenter entered the wall angle\n",
        "   - the following arrays are all (N,) where N is the number of orbits\n",
        "    - `number`: crossing number at the start of each orbit (from 0, in steps of 2)\n",
        "    - `time`: time at the start of each orbit (from 0, in steps of 1 period)\n",
        "    - `maxjaxis`: fit major axis (in mm) of each orbit\n",
        "    - `minaxis`: fit minor axis (in mm) of each orbit\n",
        "    - `theta`: angle (in radians) of the major axis to the x-axis\n",
        "    - `thetaRelToWall`: angle (in radians) of the major axis rel to an axis aligned with the wall\n",
        "    - `slope`: slope of the voltage vs. time trace (in V/s) at the crossing - not used in this analysis\n",
        "  \n",
        "So to plot the angle vs time for your first experiment:\n",
        "\n",
        "```plt.plot(mydata.fits[0].time, mydata.fits[0].theta)```\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nXOAJtsGazrU"
      },
      "source": [
        "mydata = loadAllDataSets('/content/mydata/')[0] \n",
        "alldata = loadAllDataSets('/content/data/')\n",
        "\n",
        "plt.plot(mydata.fits[0].time, mydata.fits[0].theta)\n",
        "plt.title('angle vs. time for first experiment')\n",
        "plt.show()\n",
        "\n",
        "print(\"available fields in mydata: \")\n",
        "print(list(mydata.keys()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XAUhoOivZri"
      },
      "source": [
        "#Part 1 - plot the precession rate vs. the predicted precession rate for your experiments\n",
        "\n",
        "1. Choose a time bin size, $\\Delta t$ to average over. Start with `deltat = 60` seconds, but try a few different values once everything works.\n",
        "1. Create empty lists to store the following results: `predictedrate`, `measuredrate`\n",
        "1. Loop over each fit (trial) in your set (`for f in mydata.fits:`)\n",
        "    1. Calculate $\\omega = \\frac{2\\pi}{T}$ Note that the period is stored in `f.period`\n",
        "    1. Calculate the predicted precession rate (for each orbit, without foucault precession):  $\\Omega_{pred} = 0.375 * \\omega * \\frac{a b}{L^2}$ ($a$ and $b$ (in mm) for each orbit are stored in `f.majaxis` and `f.minaxis` and $L$ (in mm) is stored in f.length). Store the prediction in a variable called `omega_pred`\n",
        "    1. Calculate the measured precession rate as $\\frac{d \\theta}{d t}$. Given $t,y$, you can calculate $\\frac{dy}{dt}$ using the function `rateOfChange(t,y)`. Note that time and theta are both stored in `f.time` and `f.theta`. Store the measured rate in a variable called `omega_meas`\n",
        "    1. For both the predicted and measured rates, calculate the means over time, using `meanOverTime` with your stored value of `deltat`\n",
        "    1. Append the mean-over-time predicted and measured rates to predictedrate and measuredrate. \n",
        "\n",
        "\n",
        "1. Convert your lists of fit values to a single array: `pred = np.concatenate(predictedrate)`, `meas = np.concatenate(measuredrate)`\n",
        "1. Use `plt.scatter` to make a scatter plot of measured (on y-axis) vs. predicted (on x-axis) precession rates. Muiltiply by degrees per hr (`deghr`) before plotting so the axes are reasonable to read. Add  `label = data` as a keywork argument to the plot command.  \n",
        "1. Plot the prediction of the model and foucault precession: `plt.plot(pred*deghr, (pred + omega_foucault)*deghr, 'm--', label = 'predicted')` as a dashed magenta line\n",
        "1. Fit the measured rate to the predicted rate (along with the errors in the fit parameters) ($\\Omega_{meas} = m*\\Omega_{pred} + b$) using `(m,b,me,be) = fitLine(...)` and plot the fit as a black solid line. Make sure to multiply by `deghr`. Add `label = 'm = m+/-me ; b = b+/-be'` (with the results from your fit -- hint use string formatting commands) Multiply b and be in the label by deghr. \n",
        "1. Use `plt.legend()` to make a legend\n",
        "1. Add x and y labels. Here's how I made my x-label `plt.set_xlabel(r'predicted: $\\frac{3}{8}\\omega \\frac{a b}{L^2}$ (deg/hr)')`\n",
        "\n",
        "1. On a separate plot, make a histogram of the residuals (measured value - fit value) in degrees per hour. Print out the standard deviation of the residuals. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rmcUb4LWbdjM"
      },
      "source": [
        "#Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq_NsZ0N7gjg"
      },
      "source": [
        "#Questions\n",
        "\n",
        "1. To what extent are your data consistent with the theory? Do you see qualitative agreement (i.e. the direction matches the predicted direction and depends on the size of $ab/L^2$)? Is the measured rate linear in $ab/L^2$ ? Are the slope and intercepts what you would expect?\n",
        "\n",
        "1. Examining the plots of the data and of the residuals, do you see any evidence of \"outliers,\" measurements that are much farther from the best fit line than you would expect given the other measurements?\n",
        "\n",
        "1. If you didn't already, play with `deltat` (try 10, 100 and 300 - don't go below 3 seconds or you might see an error if a time interval has no crossings) and examine how the graph and the fit change. Note any observations below:\n",
        "  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nbAVJ5e9D-c1"
      },
      "source": [
        "#Everyone's data\n",
        "Now let's apply the same procedure to look at the whole class data set. You should be able to basically reuse your code above. Just wrap the part where you analyze the fits into two loops\n",
        "\n",
        "```python\n",
        "ratepred = []\n",
        "ratemeas = []\n",
        "deltat = 60\n",
        "for data in alldata:\n",
        "  for f in data.fits:\n",
        "    ....\n",
        "    ....\n",
        "pred = np.concatenate(predictedrate)\n",
        "meas = np.concatenate(measuredrate)\n",
        "....\n",
        "....\n",
        "```\n",
        "\n",
        "At the end, you should produce the same two plots for everyone's data as you did for your own data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-riQyc2hM_wW"
      },
      "source": [
        "#Your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PblN7P8Iacdg"
      },
      "source": [
        "#Deal with possible outliers\n",
        "\n",
        "If you look at the class data, you might see the line is being \"pulled up\" (or \"pulled down\") by some measurements with very large predicted and measured rates.\n",
        "\n",
        "1. One way to deal with this would be to focus only on a smaller range. IE analyze the data where the predicted rate is between -200 and 200 deg/hr. \n",
        "1. There are also robust fitting strategies you can use. I've written function `fitLineRansac` and `fitLineHuber` that use these to identify \"outliers\" - data you discard as unlikely to be correctly measured/part of the same data set. \n",
        "\n",
        "Please...\n",
        "\n",
        "1. Plot all the data as a scatter plot overlaid with the linear fit in black and the prediction as a dashed magenta line (slope = 1, intercept = -9.8 deg/hr). Add the appropriate legend (data, predicted, m = m+/- m_e, b = b+/- b_e). This just duplicates your previous answer. \n",
        "1. Make a second figure. Divide the data by whether the absolute value of the predicted rate is more or less than 200 deg/hr. Plot the \"inliers\" (abs < 200) in blue and the \"outliers\" in red. Overlay this figure with the linear fit to only the inliers. Add the appropriate legend (data, predicted, m = m+/- m_e, b = b+/- b_e).\n",
        "1. Make a third figure. Use the `fitLineHuber` function to estimate the model parameters and identify inliers and outliers. As before, plot the inliers in blue, the outliers in red, and overlay the figure with the linear fit to only the inliers. Add the appropriate legend (data, predicted, m = m+/- m_e, b = b+/- b_e).\n",
        "1. Make a fourth figure that's the same as the third, except use the `fitLineRanac` function. Add the appropriate legend (data, predicted, m = m+/- m_e, b = b+/- b_e).\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#your code here"
      ],
      "metadata": {
        "id": "o1hHGYiiyHpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ni8VhjpohEc-"
      },
      "source": [
        "#Questions\n",
        "\n",
        "1. To what extent are your data consistent with the theory? Do you see qualitative agreement (i.e. the direction matches the predicted direction and depends on the size of $ab/L^2$)? Is the measured rate linear in $ab/L^2$ ? Are the slope and intercepts what you would expect?\n",
        "\n",
        "1. Do the outliers identified by the algorithms make sense to you? The fitLineHuber takes an optional argument (epsilon=...) that sets how strictly outliers are excluded. I've set the default to 2. Try 1.35 (the toolkit default). Are most of the excluded data truly outliers? If your goal is to make an accurate estimate of the slope, what are the tradeoffs of excluding more outlying data?\n",
        "\n",
        "1. If your analysis agrees with mine, and if the results hold up over the final week of data collection, it seems like the slope ends up being not exactly 1. Do you think this difference can be explained by random errors? Look both at the error reported by FitLine and at the overall trend in the data. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Check on one possible signature of a systematic error\n",
        "\n",
        "One thing we might consider is whether the pipes on the wall or some other feature of the room is causing an error in either the movement of the bob or our measurement of the orbit. \n",
        "\n",
        "As a start, let's sort the data into orbits that are parallel or perpendicular to the wall. We can (very roughly) split the data into two groups based on whether\n",
        "\n",
        "\n",
        "abs(cos$(\\theta-\\theta_{wall})) > 1/\\sqrt{2}$ (parallel)\n",
        "\n",
        "or\n",
        "\n",
        "abs(cos$(\\theta-\\theta_{wall})) < 1/\\sqrt{2}$ (perpendicular)\n",
        "\n",
        "The fit structure includes data to help you out\n",
        "\n",
        "- `hasAngleToWall`: (True/False) whether the original experimenter entered the wall angle\n",
        "\n",
        "- `thetaRelToWall`: angle (in radians) of the major axis rel to an axis aligned with the wall shape: (N,)\n",
        "\n",
        "So now, please repeat your previous analyses separately for orbits parallel and perpendicular to the wall. I'll put all the steps below, with the new parts highlighted\n",
        "\n",
        "\n",
        "1. Choose a time bin size, $\\Delta t$ to average over. Start with `deltat = 60` seconds, but try a few different values once everything works.\n",
        "1. **CHANGED** Create empty lists to store the following results: `predictedrate_parallel`, `measuredrate_parallel`,`predictedrate_perpendicular`, `measuredrate_perpendicular` \n",
        "1. Loop over all data sets (`for data in alldata:`)\n",
        "  1. Loop over each fit (trial) in your set (`for f in data.fits:`)\n",
        "    1. Calculate $\\omega = \\frac{2\\pi}{T}$ Note that the period is stored in `f.period`\n",
        "    1. Calculate the predicted precession rate (for each orbit, without foucault precession):  $\\Omega_{pred} = 0.375 * \\omega * \\frac{a b}{L^2}$ ($a$ and $b$ (in mm) for each orbit are stored in `f.majaxis` and `f.minaxis` and $L$ (in mm) is stored in f.length). Store the prediction in a variable called `omega_pred`\n",
        "    1. Calculate the measured precession rate as $\\frac{d \\theta}{d t}$. Given $t,y$, you can calculate $\\frac{dy}{dt}$ using the function `rateOfChange(t,y)`. Note that time and theta are both stored in `f.time` and `f.theta`. Store the measured rate in a variable called `omega_meas`\n",
        "    1. For both the predicted and measured rates, calculate the means over time, using `meanOverTime` with your stored value of `deltat`. Store these in `mot_pred` and `mot_meas`\n",
        "    1. **NEW** Calculate the average of `np.cos(f.thetaRelToWall)` over time, using `meanOverTime` with your stored value of `deltat`. Store this in `mot_thetaToWall`\n",
        "    1. **NEW** define a new variable `parallel = np.logical_and(f.hasAngleToWall,np.abs(mot_thetaToWall) > 1/np.sqrt(2))`\n",
        "    1. **NEW** define a new variable `perpendicular` that is true if `f.hasAngletoWall` is true and $ |\\cos(\\theta-\\theta_{wall})| < 1/\\sqrt{2}$. Hint: this is NOT just `np.logical_not(parallel)`. \n",
        "    1. **CHANGED** Append `mot_pred[parallel]` to `predictedrate_parallel`, `mot_meas[parallel]` to `measuredrate_parallel` and so on. \n",
        "\n",
        "1. **CHANGED** Convert your lists of fit values to a single array: `pred_parallel = np.concatenate(predictedrate_parallel)`, `meas_perpendicular = np.concatenate(measuredrate_perpendicular)` and so on\n",
        "\n",
        "1. **CHANGED** Do the following separately for the parallel and perpendicular data sets your created.\n",
        "  1. Use `plt.scatter` to make a scatter plot of measured (on y-axis) vs. predicted (on x-axis) precession rates. Multiply by degrees per hr (`deghr`) before plotting so the axes are reasonable to read. Add  `label = data` as a keywork argument to the plot command.  \n",
        "  1. Plot the prediction of the model and foucault precession: `plt.plot(pred*deghr, (pred + omega_foucault)*deghr, 'm--', label = 'predicted')` as a dashed magenta line\n",
        "  1. Fit the measured rate to the predicted rate ($\\Omega_{meas} = m*\\Omega_{pred} + b$) using `np.polyfit` and plot the fit as a black solid line. Make sure to multiply by `deghr`. Add `label = 'measured'` to the command. \n",
        "  1. Use `plt.legend()` to make a legend\n",
        "  1. Add x and y labels. Here's how I made my x-label `plt.set_xlabel(r'predicted: $\\frac{3}{8}\\omega \\frac{a b}{L^2}$ (deg/hr)')`\n",
        "  1. **NEW** Title the plot `parallel to wall` or `perpendicular to wall`\n",
        "  1. Print out the following information: **parallel or perpendicular:**,  slope of the linear fit between predicted (x) and measured (y) values (would be 1 if measured rates exactly match predicted rates); 2 intercept of the linear fit, multiplied by deghr (would be -9.8 deg/hr if only perturbing influence is foucault precession)\n"
      ],
      "metadata": {
        "id": "GQWuY153NTB-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#your code here"
      ],
      "metadata": {
        "id": "m8kUeTHbXZ5W"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}